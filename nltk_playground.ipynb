{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1431d80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>pulishedDate</th>\n",
       "      <th>pageCount</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>full_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>SUMMARY - Bullshit Jobs: A Theory By David Gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Shortcut Edition']</td>\n",
       "      <td>2021-06-17</td>\n",
       "      <td>24</td>\n",
       "      <td>['Business &amp; Economics']</td>\n",
       "      <td>Our summary is short simple and pragmatic It ...</td>\n",
       "      <td>SUMMARY - Bullshit Jobs: A Theory By David Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The Science of Self-Learning</td>\n",
       "      <td>How to Teach Yourself Anything, Learn More in ...</td>\n",
       "      <td>['Peter Hollins']</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>202</td>\n",
       "      <td>['Education']</td>\n",
       "      <td>How to learn effectively when you have to be b...</td>\n",
       "      <td>The Science of Self-Learning How to Teach Your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Zen Golf</td>\n",
       "      <td>Mastering the Mental Game</td>\n",
       "      <td>['Joseph Parent']</td>\n",
       "      <td>2002-06-18</td>\n",
       "      <td>226</td>\n",
       "      <td>['Sports &amp; Recreation']</td>\n",
       "      <td>A highly original and groundbreaking book from...</td>\n",
       "      <td>Zen Golf Mastering the Mental Game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Mighty Numbers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Marvel Press Book Group']</td>\n",
       "      <td>2016-05-03</td>\n",
       "      <td>0</td>\n",
       "      <td>['Juvenile Fiction']</td>\n",
       "      <td>Young children will learn to count from one to...</td>\n",
       "      <td>Mighty Numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Happier Hour</td>\n",
       "      <td>How to Beat Distraction, Expand Your Time, and...</td>\n",
       "      <td>['Cassie Holmes']</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>320</td>\n",
       "      <td>['Biography &amp; Autobiography']</td>\n",
       "      <td>We live in a culture where most of us suffer f...</td>\n",
       "      <td>Happier Hour How to Beat Distraction, Expand Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "3           0  SUMMARY - Bullshit Jobs: A Theory By David Gra...   \n",
       "4           0                       The Science of Self-Learning   \n",
       "5           0                                           Zen Golf   \n",
       "6           0                                     Mighty Numbers   \n",
       "7           0                                       Happier Hour   \n",
       "\n",
       "                                            subtitle  \\\n",
       "3                                                NaN   \n",
       "4  How to Teach Yourself Anything, Learn More in ...   \n",
       "5                          Mastering the Mental Game   \n",
       "6                                                NaN   \n",
       "7  How to Beat Distraction, Expand Your Time, and...   \n",
       "\n",
       "                       authors pulishedDate  pageCount  \\\n",
       "3         ['Shortcut Edition']   2021-06-17         24   \n",
       "4            ['Peter Hollins']   2019-10-22        202   \n",
       "5            ['Joseph Parent']   2002-06-18        226   \n",
       "6  ['Marvel Press Book Group']   2016-05-03          0   \n",
       "7            ['Cassie Holmes']   2023-06-20        320   \n",
       "\n",
       "                      categories  \\\n",
       "3       ['Business & Economics']   \n",
       "4                  ['Education']   \n",
       "5        ['Sports & Recreation']   \n",
       "6           ['Juvenile Fiction']   \n",
       "7  ['Biography & Autobiography']   \n",
       "\n",
       "                                         description  \\\n",
       "3   Our summary is short simple and pragmatic It ...   \n",
       "4  How to learn effectively when you have to be b...   \n",
       "5  A highly original and groundbreaking book from...   \n",
       "6  Young children will learn to count from one to...   \n",
       "7  We live in a culture where most of us suffer f...   \n",
       "\n",
       "                                          full_title  \n",
       "3  SUMMARY - Bullshit Jobs: A Theory By David Gra...  \n",
       "4  The Science of Self-Learning How to Teach Your...  \n",
       "5                 Zen Golf Mastering the Mental Game  \n",
       "6                                     Mighty Numbers  \n",
       "7  Happier Hour How to Beat Distraction, Expand Y...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import thefuzz as fuzz\n",
    "import re\n",
    "import string\n",
    "\n",
    "from utils import check_to_run_initial_data_load, pull_from_google_books, create_library\n",
    "from utils import titles_l # Input data\n",
    "from utils import authors_l # Input data\n",
    "\n",
    "MATCH_SCORE = 70\n",
    "LAST_N_BOOKS = 10\n",
    "TERMS_IN_SEARCH_QUERY = 7\n",
    "\n",
    "final_books_df = pd.read_csv('library.csv')\n",
    "final_books_df = final_books_df.tail(LAST_N_BOOKS)\n",
    "\n",
    "final_books_df['description'] = final_books_df['description'].astype(str).apply(\n",
    "    lambda x: re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", x)\n",
    ")\n",
    "\n",
    "final_books_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1026f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.4)\n",
      "Collecting thefuzz (from -r requirements.txt (line 4))\n",
      "  Using cached thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pyarrow (from -r requirements.txt (line 5))\n",
      "  Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from -r requirements.txt (line 6)) (6.29.5)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 7))\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting rake-nltk (from -r requirements.txt (line 8))\n",
      "  Using cached rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz->-r requirements.txt (line 4))\n",
      "  Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (9.1.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->-r requirements.txt (line 6)) (5.14.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 7))\n",
      "  Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 7))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 7))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.2 (from rake-nltk->-r requirements.txt (line 8))\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 6)) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 6)) (310)\n",
      "Collecting click (from nltk<4.0.0,>=3.6.2->rake-nltk->-r requirements.txt (line 8))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.6.2->rake-nltk->-r requirements.txt (line 8))\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk<4.0.0,>=3.6.2->rake-nltk->-r requirements.txt (line 8))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\matt\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 6)) (0.2.3)\n",
      "Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.1 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.1 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 24.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 5.2/41.0 MB 28.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.5/41.0 MB 27.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 17.0/41.0 MB 28.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 23.3/41.0 MB 29.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 29.9/41.0 MB 29.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 36.2/41.0 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 30.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 27.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, regex, rapidfuzz, pyarrow, joblib, click, thefuzz, scikit-learn, nltk, rake-nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 pyarrow-19.0.1 rake-nltk-1.0.6 rapidfuzz-3.13.0 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.15.2 thefuzz-0.22.1 threadpoolctl-3.6.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56555f2",
   "metadata": {},
   "source": [
    "## TFIDF as a method to generate Search Query - GOING WITH THIS APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad5942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords in the whole dataset: ['business', 'humor', 'time', 'book', 'golf', 'jobs', 'life']\n",
      "business humor time book golf jobs life\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X = tfidf.fit_transform(final_books_df['description'])\n",
    "\n",
    "# Get feature names and sum TF-IDF scores across all documents\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = X.sum(axis=0).A1  # Flatten the matrix to 1D array\n",
    "\n",
    "# Get indices of top 7 keywords\n",
    "top_indices = tfidf_scores.argsort()[-TERMS_IN_SEARCH_QUERY:][::-1]\n",
    "top_keywords = [feature_names[i] for i in top_indices]\n",
    "\n",
    "print(\"Top keywords in the whole dataset:\", top_keywords)\n",
    "tfidf_search_query = \" \".join(top_keywords)\n",
    "print(tfidf_search_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fe003",
   "metadata": {},
   "source": [
    "##  Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101adeca",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\matt/nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\matt\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m corpus = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(final_books_df[\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[32m      5\u001b[39m rake = Rake(max_length=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrake\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m keywords = rake.get_ranked_phrases()[:TERMS_IN_SEARCH_QUERY]\n\u001b[32m      9\u001b[39m rake_search_query = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(keywords)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\rake_nltk\\rake.py:126\u001b[39m, in \u001b[36mRake.extract_keywords_from_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_keywords_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Method to extract keywords from the text provided.\u001b[39;00m\n\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    :param text: Text to extract keywords from, provided as a string.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     sentences: List[Sentence] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize_text_to_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m.extract_keywords_from_sentences(sentences)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\rake_nltk\\rake.py:180\u001b[39m, in \u001b[36mRake._tokenize_text_to_sentences\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tokenize_text_to_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[Sentence]:\n\u001b[32m    173\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Tokenizes the given text string into sentences using the configured\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    sentence tokenizer. Configuration uses `nltk.tokenize.sent_tokenize`\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m    by default.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \u001b[33;03m    :return: List of sentences as per the tokenizer used.\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentence_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\matt/nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\matt\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\matt\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "corpus = \" \".join(final_books_df['description'].astype(str))\n",
    "\n",
    "rake = Rake(max_length=2)\n",
    "rake.extract_keywords_from_text(corpus)\n",
    "keywords = rake.get_ranked_phrases()[:TERMS_IN_SEARCH_QUERY]\n",
    "\n",
    "rake_search_query = \" \".join(keywords)\n",
    "print(rake_search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f811760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2efea1e9",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffc5a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'business', 'humor', 'life', 'golf', 'time', 'authors']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = \" \".join(final_books_df['description'].astype(str))\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "X = vectorizer.fit_transform([corpus])\n",
    "sum_words = X.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_keywords = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_keywords = [word for word, freq in sorted_keywords[:TERMS_IN_SEARCH_QUERY]]\n",
    "count_search_query = \" \".join(top_keywords)\n",
    "\n",
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f530a7",
   "metadata": {},
   "source": [
    "## SpaCy - For the life of me I cannot get this to install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f09e4",
   "metadata": {},
   "source": [
    "## KeyBERT (BERT w/ cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa01db80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work smarter dumb jobs stupid jobs jobs useless jobs prove jobs consequences consequences jobs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "corpus = \" \".join(final_books_df['description'].astype(str))\n",
    "kw_model = KeyBERT()\n",
    "bert_keywords = kw_model.extract_keywords(corpus,\n",
    "                                          keyphrase_ngram_range=(1, 2),\n",
    "                                          top_n=TERMS_IN_SEARCH_QUERY)\n",
    "\n",
    "bert_keywords_ = [word for word, freq in bert_keywords[:TERMS_IN_SEARCH_QUERY]]\n",
    "bert_search_query = \" \".join(bert_keywords_)\n",
    "bert_search_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f4384",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c9e03bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generational',\n",
       " 'generation',\n",
       " 'author',\n",
       " 'authors',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install summa\n",
    "from summa import keywords\n",
    "\n",
    "corpus = \" \".join(final_books_df['description'].astype(str))\n",
    "summa_extracted_keywords = keywords.keywords(corpus, split=True)\n",
    "summa_extracted_keywords[:TERMS_IN_SEARCH_QUERY]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
